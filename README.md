# SUPERT: Unsupervised Multi-Document Summarization Evaluation & Generation

This branch provides the code for comparing the performance of different unsupervised metrics. Results presented in Tables 1 - 4 in the original [paper](https://arxiv.org/abs/2005.03724) were generated by the code in this branch.

## How to Set Up 
* Prerequisite: Python 3.6 or higher versions
* Download TAC08 from [here](https://tac.nist.gov/data/past/2008/UpdateSumm08.html) and TAC09 from [here](https://tac.nist.gov/data/past/2009/Summ09.html), and put the downloaded data to the directory *data/*.
* Install all packages in *requirement.txt*.
```shell script
pip3 install -r requirements.txt
```

## How to run

* Code for the baseline methods (i.e. metrics presented in Table 1) are at *baseline_score*. 
For example, to test the performance of REAPER, run the command lines below:
```shell script
cd baseline_score
python reaper_reward.py 
```
* Code for the embedding-based metrics (i.e. metrics presented in Table 2 - 4) are at *ref_free_metrics/similarity_measurements*. For example, to test the performance of ELMo-based metrics, run the command lines below:
```shell script
cd ref_free_metrics/similarity_measurements
python elmo_metrics.py 
```
In the main function of each file, you can change the dataset (TAC08 or 09) and the 
strategy for generating pseudo references.


## License
Apache License Version 2.0
